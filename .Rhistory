# Point in polygon function
polys$pt_count <- lengths(st_intersects(polys, points))
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
#municipalities <- st_make_valid(polys) %>% group_by(NMUN) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
income$Renta_Persona <- as.numeric(income$Renta_Persona)
income$Renta_Hogar <- as.numeric(income$Renta_Hogar)
income <- income %>% separate(Area, c("CUSEC", "NMUN"))
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(ggplot2)
ggplot(polys,aes(Renta_Hogar,pt_count))
ggplot(polys,aes(Renta_Hogar,pt_count)) + geom_point()
# Twitter data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets
points <- data %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys$tw_count <- lengths(st_intersects(polys, tweets))
summary(polys)
ggplot(polys,aes(tw_count,pt_count)) + geom_point()
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(data), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
ggplot(districts,aes(tw_pct,bt_pct)) + geom_point()
districts$more_tweets <- districts$tw_pct - districts$bt_pct
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point()
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
ggplot(filter(districts, tw_count>0),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
filter(districts, tw_count>0),
filter(districts, tw_count>0)
filter(districts, tw_count>10)
ggplot(filter(districts, tw_count>10),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
View(districts)
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
View(tweets)
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
View(districts)
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$more_tweets <- districts$tw_pct - districts$bt_pct
# Load and add income data to district data
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN2"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Collapse by community or municipality
#communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise(bt_count = sum(bt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
munitweets <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(tw_count = sum(tw_count))
municipalities <- municipalities %>% left_join(munitweets, by = "NMUN")
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
municipalities
municipalities$bt_pct <- municipalities$bt_count/sum(municipalities$bt_count)
municipalities$tw_pct <- municipalities$tw_count/sum(municipalities$tw_count)
municipalities$more_tweets <- municipalities$tw_pct - municipalities$bt_pct
View(municipalities)
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$more_tweets <- communities$tw_pct - communities$bt_pct
View(communities)
communities
communities %>% arrange(more_tweets)
municipalities %>% arrange(more_tweets)
# CSV exports
write_csv(communities, "~/research/communities.csv")
write_csv(municipalities, "~/research/municipalities.csv")
municipalities %>% arrange(desc(more_tweets)
)
# Names of all required packages that we will be using during the course
all_pkgs <- c('tidyverse', 'jtools', 'sjPlot','ggplot2','ggthemes','haven','foreign','essurvey','stargazer','knitr','prais','orcutt','fastDummies')
# Install all the packages (it may take a few minutes to install all of them)
install.packages(all_pkgs, dependencies = TRUE)
# Verify the installation worked correctly
setdiff(all_pkgs, row.names(installed.packages()))
library(essurvey)
?essurvey
# FutStat.cat
source("~/Google Drive/Futbol/futstat.R")
k<-1
suppressMessages(library(tidyverse))
suppressMessages(library(rvest))
suppressMessages(library(goalmodel))
suppressMessages(library(scales))
suppressMessages(library(blogdown))
suppressMessages(library(rmarkdown))
options(dplyr.summarise.inform=F)
setwd("~/Google Drive/Futbol")
# Set parameters
year <- 2021
xi.set <- 0.0025
# Read list of leagues
leagues <- read_csv("leagues.csv", col_types = cols())
# Select leagues to be updated
leagues <- leagues[1:2,] # Estiu 2020
# Blank datasets
aa.final.table <- NA
aa.predictions <- NA
aa.all.projs <- NA # If re-running trajectories
trajstart <- 1 # If re-running trajectories
#aa.all.projs <- read_csv("data_trajectories.csv", col_types = cols()) # Remove if re-running trajectories
#trajstart <- aa.all.projs %>% group_by(league) %>% summarise(round = max(round))
#trajstart <- min(trajstart$round)-1
#aa.all.projs <- aa.all.projs %>% filter(round < trajstart) # And fix i statement (line ~224)
lg.name <- as.character(leagues[k,1])
lg.code <- as.character(leagues[k,2])
lg.group <- as.numeric(leagues[k,3])
lg.type1 <- as.numeric(leagues[k,4])
lg.type2 <- as.numeric(leagues[k,5])
lg.type3 <- as.numeric(leagues[k,6])
lg.type1name <- as.character(leagues[k,7])
lg.type2name <- as.character(leagues[k,8])
print(paste0("Now running: ",lg.name," || League ",k," of ", nrow(leagues)))
print("Scraping...")
# Scrape page
webpage <- read_html(paste0("https://www.resultados-futbol.com/",lg.code,year,"/grupo",lg.group,"/calendario"))
local_data <- html_nodes(webpage,'.equipo1') %>%  html_nodes("a") %>% html_attr("href")
vist_data <- html_nodes(webpage,'.equipo2') %>%  html_nodes("a") %>% html_attr("href")
result_data <- html_nodes(webpage,'.rstd') %>% html_text()
dates_data <- html_nodes(webpage,'.fecha') %>% html_text()
# Create dataset of all games
games <- data.frame(home = local_data, away = vist_data, score = result_data, date = dates_data)
games$score <- str_sub(games$score, -11, -7)
# Recode dates
games <- games %>% separate(date, c("day", "month", "year"), extra = "drop")
games$year <- as.numeric(games$year) + 2000
month <- c("Ago", "Sep", "Oct", "Nov", "Dic", "Ene", "Feb", "Mar", "Abr", "May", "Jun", "Jul")
month.num <- c(8,9,10,11,12,1,2,3,4,5,6,7)
month.recode <- cbind.data.frame(month,month.num)
games <- games %>% left_join(month.recode, by = "month")
games$date <- as.Date(with(games, paste(year, month.num, day,sep="-")), "%Y-%m-%d")
games <- games %>% select(-day, -month, -year, -month.num)
# Add jornada
teams <- length(unique(games$home))
jornadas <- (teams-1)*2
games$jornada <- rep(1:jornadas, each = (teams/2))
# Separate compeleted and uncompleted games
schedule <- games %>% filter(grepl("x|:|A", games$score) | grepl("x|'|A", games$score) | grepl("x|D|A", games$score)) %>% select(-score)
results <- filter(games, grepl("-", games$score))
results <- filter(results, !grepl("x", results$score))
results <- filter(results, !grepl("'", results$score))
results <- results %>% separate(score, c("home.score", "away.score"), extra = "drop")
results$home.score <- as.numeric(results$home.score)
results$away.score <- as.numeric(results$away.score)
# Add in historic results
clublist <- as.character(unique(games$home))
history <- read_csv("gameshistory.csv", col_types = cols()) %>% select(-season,-league)
history <- history %>% filter(home %in% clublist & away %in% clublist)
histresults <- rbind(history,results)
# Create points variables
results <- mutate(results, home.points = ifelse(home.score > away.score, 3,
ifelse(home.score == away.score, 1, 0)))
results <- mutate(results, away.points = ifelse(home.score < away.score, 3,
ifelse(home.score == away.score, 1, 0)))
print("Simulating...")
# Model
weights <- weights_dc(histresults$date, xi=xi.set)
model <- goalmodel(goals1 = histresults$home.score, goals2 = histresults$away.score,
team1 = histresults$home, team2=histresults$away, weights = weights, rs=TRUE)
# FutStat.cat
source("~/Google Drive/Futbol/futstat.R")
summary(model)
unique(schedule$home)
unique(schedule$away)
k<-1
suppressMessages(library(tidyverse))
suppressMessages(library(rvest))
suppressMessages(library(goalmodel))
suppressMessages(library(scales))
suppressMessages(library(blogdown))
suppressMessages(library(rmarkdown))
options(dplyr.summarise.inform=F)
setwd("~/Google Drive/Futbol")
# Set parameters
year <- 2021
xi.set <- 0.0025
# Read list of leagues
leagues <- read_csv("leagues.csv", col_types = cols())
# Select leagues to be updated
leagues <- leagues[1:2,] # Estiu 2020
# Blank datasets
aa.final.table <- NA
aa.predictions <- NA
aa.all.projs <- NA # If re-running trajectories
trajstart <- 1 # If re-running trajectories
#aa.all.projs <- read_csv("data_trajectories.csv", col_types = cols()) # Remove if re-running trajectories
#trajstart <- aa.all.projs %>% group_by(league) %>% summarise(round = max(round))
#trajstart <- min(trajstart$round)-1
#aa.all.projs <- aa.all.projs %>% filter(round < trajstart) # And fix i statement (line ~224)
lg.name <- as.character(leagues[k,1])
lg.code <- as.character(leagues[k,2])
lg.group <- as.numeric(leagues[k,3])
lg.type1 <- as.numeric(leagues[k,4])
lg.type2 <- as.numeric(leagues[k,5])
lg.type3 <- as.numeric(leagues[k,6])
lg.type1name <- as.character(leagues[k,7])
lg.type2name <- as.character(leagues[k,8])
print(paste0("Now running: ",lg.name," || League ",k," of ", nrow(leagues)))
print("Scraping...")
# Scrape page
webpage <- read_html(paste0("https://www.resultados-futbol.com/",lg.code,year,"/grupo",lg.group,"/calendario"))
local_data <- html_nodes(webpage,'.equipo1') %>%  html_nodes("a") %>% html_attr("href")
vist_data <- html_nodes(webpage,'.equipo2') %>%  html_nodes("a") %>% html_attr("href")
result_data <- html_nodes(webpage,'.rstd') %>% html_text()
dates_data <- html_nodes(webpage,'.fecha') %>% html_text()
# Create dataset of all games
games <- data.frame(home = local_data, away = vist_data, score = result_data, date = dates_data)
games$score <- str_sub(games$score, -11, -7)
# Recode dates
games <- games %>% separate(date, c("day", "month", "year"), extra = "drop")
games$year <- as.numeric(games$year) + 2000
month <- c("Ago", "Sep", "Oct", "Nov", "Dic", "Ene", "Feb", "Mar", "Abr", "May", "Jun", "Jul")
month.num <- c(8,9,10,11,12,1,2,3,4,5,6,7)
month.recode <- cbind.data.frame(month,month.num)
games <- games %>% left_join(month.recode, by = "month")
games$date <- as.Date(with(games, paste(year, month.num, day,sep="-")), "%Y-%m-%d")
games <- games %>% select(-day, -month, -year, -month.num)
# Add jornada
teams <- length(unique(games$home))
jornadas <- (teams-1)*2
games$jornada <- rep(1:jornadas, each = (teams/2))
# Separate compeleted and uncompleted games
schedule <- games %>% filter(grepl("x|:|A", games$score) | grepl("x|'|A", games$score) | grepl("x|D|A", games$score)) %>% select(-score)
results <- filter(games, grepl("-", games$score))
results <- filter(results, !grepl("x", results$score))
results <- filter(results, !grepl("'", results$score))
results <- results %>% separate(score, c("home.score", "away.score"), extra = "drop")
results$home.score <- as.numeric(results$home.score)
results$away.score <- as.numeric(results$away.score)
# Add in historic results
clublist <- as.character(unique(games$home))
history <- read_csv("gameshistory.csv", col_types = cols()) %>% select(-season,-league)
history <- history %>% filter(home %in% clublist & away %in% clublist)
histresults <- rbind(history,results)
# Create points variables
results <- mutate(results, home.points = ifelse(home.score > away.score, 3,
ifelse(home.score == away.score, 1, 0)))
results <- mutate(results, away.points = ifelse(home.score < away.score, 3,
ifelse(home.score == away.score, 1, 0)))
print("Simulating...")
# Model
weights <- weights_dc(histresults$date, xi=xi.set)
model <- goalmodel(goals1 = histresults$home.score, goals2 = histresults$away.score,
team1 = histresults$home, team2=histresults$away, weights = weights, rs=TRUE)
# Set up results for sims
if (nrow(schedule) > 0){n.sim <- 10000}
if (nrow(schedule) == 0){n.sim <- 1}
results.for.sims <- data.frame(results, sim.number = rep(1:n.sim, each = nrow(results)))
results.for.sims <- results.for.sims %>% select(home, away, sim.number, home.points, away.points, home.score, away.score)
names(results.for.sims) <- c("team1","team2","sim.number","team1.points","team2.points","team1.score","team2.score")
# Simulate unplayed games
if (nrow(schedule) > 0){
predictions <- predict_result(model, team1=schedule$home, team2=schedule$away, return_df = TRUE)
sims <- data.frame(predictions, sim.number = rep(1:n.sim, each = nrow(predictions)))
sims$random <- runif((nrow(sims)),0,1)
sims <- mutate(sims, team1.points = ifelse(random < p1, 3,
ifelse(random > (1-p2), 0, 1)))
sims <- mutate(sims, team2.points = ifelse(team1.points == 0, 3,
ifelse(team1.points == 3, 0, 1)))
# Add in known results
sim.seasons <- sims %>% select(team1,team2,sim.number,team1.points,team2.points)
sim.seasons$team1.score <- sim.seasons$team1.points/2
sim.seasons$team2.score <- sim.seasons$team2.points/2
sim.seasons <- rbind(sim.seasons,results.for.sims)
}
if (nrow(schedule) == 0){sim.seasons <- results.for.sims}
# Add in opposite leg (home/away) for each matchup
sim.seasons$matchup <- paste0(sim.seasons$team1,sim.seasons$team2,sim.seasons$sim.number)
legs <- sim.seasons
legs$matchup <- paste0(sim.seasons$team2,sim.seasons$team1,sim.seasons$sim.number)
legs <- legs %>% select(-team1,-team2,-sim.number)
names(legs) <- c("team2.points.leg","team1.points.leg","team2.score.leg","team1.score.leg","matchup")
sim.seasons <- sim.seasons %>% left_join(legs, by = "matchup")
# Tally matchup points and goal differential
sim.seasons$team1.matchup.points <- sim.seasons$team1.points + sim.seasons$team1.points.leg
sim.seasons$team1.matchup.gd <- sim.seasons$team1.score + sim.seasons$team1.score.leg - sim.seasons$team2.score - sim.seasons$team2.score.leg
# Matchups as dataframe
matchups <- sim.seasons %>% select(matchup,team1.matchup.points,team1.matchup.gd)
# Create table for each simulated season
sim.tables <- sim.seasons %>% group_by(team1,sim.number) %>% summarize(points = sum(team1.matchup.points)) %>% arrange(desc(points)) %>% arrange(sim.number)
# Create first and second tiebreakers
sim.tables$tie.team <- c(rep(NA, 1), as.character(sim.tables$team1))[1 : length(sim.tables$team1)]
sim.tables$tie.points <- c(rep(NA, 1), sim.tables$points)[1 : length(sim.tables$points)]
sim.tables$matchup <- paste0(sim.tables$team1,sim.tables$tie.team,sim.tables$sim.number)
sim.tables <- sim.tables %>% left_join(matchups, by = "matchup")
sim.tables$tiebreak1 <- ifelse(sim.tables$points == sim.tables$tie.points, sim.tables$team1.matchup.points, 2)
sim.tables$tiebreak2 <- ifelse(sim.tables$points == sim.tables$tie.points, sim.tables$team1.matchup.gd, 0)
sim.tables$team <- sim.tables$team1
sim.tables <- sim.tables %>% ungroup() %>% select(team,sim.number, points, tiebreak1, tiebreak2)
# Add third tiebreaker (goal differential) and sort
gd <- predict_expg(model, team1=schedule$home, team2=schedule$away, return_df = TRUE)
gd$t1gd <- gd$expg1-gd$expg2
gd$t2gd <- gd$expg2-gd$expg1
results$home.gd <- results$home.score-results$away.score
results$away.gd <- results$away.score-results$home.score
lg.gd <- as_tibble_col(c(as.character(results$home),as.character(results$away),as.character(gd$team1), as.character(gd$team2)),column_name = "team")
lg.gd$gd <- c(as.numeric(results$home.gd),as.numeric(results$away.gd),as.numeric(gd$t1gd), as.numeric(gd$t2gd))
lg.gd <- lg.gd %>% group_by(team) %>% summarize(gd = sum(gd))
sim.tables <- sim.tables %>% left_join(lg.gd, by = "team")
sim.tables <- sim.tables %>% arrange(desc(gd)) %>% arrange(desc(tiebreak2)) %>% arrange(desc(tiebreak1)) %>% arrange(desc(points)) %>% arrange(sim.number)
# Add rank
sim.tables$rank <- rep(1:teams, times = (nrow(sim.tables)/teams))
type1 <- sim.tables %>% filter(rank <= lg.type1) %>%
group_by(team) %>% count() %>% mutate(type1 = (n/n.sim)*100) %>% select(-n) %>% ungroup()
type2 <- sim.tables %>% filter(rank < lg.type2) %>%
group_by(team) %>% count() %>% mutate(type2 = (n/n.sim)*100) %>% select(-n) %>% ungroup()
type3 <- sim.tables %>% filter(rank > lg.type3) %>%
group_by(team) %>% count() %>% mutate(type3 = (n/n.sim)*100) %>% select(-n) %>% ungroup()
typex <- sim.tables %>% filter(rank == 5 | rank == 6 | rank == 7) %>%
group_by(team) %>% count() %>% mutate(typex = (n/n.sim)*100) %>% select(-n) %>% ungroup()
avgrank <- sim.tables %>% group_by(team) %>% summarize(avgrank = mean(rank)) %>% ungroup()
print("Simulation Complete")
# Create xGols
rate.sked <- results %>% select(home)
if (nrow(rate.sked) == 0){rate.sked <- schedule %>% select(home)}
rate.sked <- unique(rate.sked)
colnames(rate.sked) <- "team"
rate.sked$team2 <- "nobody"
n <- nrow(rate.sked)+1
xgmodel <- model
xgmodel$parameters$attack[n] <- 0
names(xgmodel$parameters$attack[n]) <- c("nobody")
newnames <- names(xgmodel$parameters$attack)
newnames[n] <- "nobody"
names(xgmodel$parameters$attack) <- newnames
xgmodel$parameters$defense[n] <- 0
newnames <- names(xgmodel$parameters$defense)
newnames[n] <- "nobody"
names(xgmodel$parameters$defense) <- newnames
xgmodel$all_teams[n] <- "nobody"
rate.preds <- predict_expg(xgmodel, team1=rate.sked$team, team2=rate.sked$team2, return_df = TRUE)
rate.preds.away <- predict_expg(xgmodel, team1=rate.sked$team2, team2=rate.sked$team, return_df = TRUE)
rate.preds.away$team1 <- rate.preds.away$team2
rate.preds <- rate.preds %>% select(-team2) %>% left_join(rate.preds.away, by = "team1")
rate.preds$expg1 <- (rate.preds$expg1.x + rate.preds$expg2.y)/2
rate.preds$expg2 <- (rate.preds$expg2.x + rate.preds$expg1.y)/2
xgols <- rate.preds %>% select(team1, expg1, expg2) %>% rename(
team = team1,
off.rank = expg1,
def.rank = expg2)
# Create final table
final.table <- sim.tables %>% group_by(team) %>% summarize(proj.points = sum(points/n.sim)) %>% arrange(desc(proj.points))
final.table <- final.table %>% left_join(lg.gd, by = "team")
final.table <- final.table %>% left_join(type1, by = "team")
final.table <- final.table %>% left_join(type2, by = "team")
final.table <- final.table %>% left_join(typex, by = "team")
final.table <- final.table %>% left_join(type3, by = "team")
final.table <- final.table %>% left_join(xgols, by = "team")
final.table <- final.table %>% left_join(avgrank, by = "team")
final.table <- final.table %>% arrange(avgrank) %>% select(-avgrank)
final.table$qualitat <- (final.table$off.rank - final.table$def.rank + 3)*20
final.table[is.na(final.table)] <- 0
summary(xgmodel)
rate.sked
# Create xGols
rate.sked <- clublist
# Create xGols
rate.sked <- data.frame(clublist)
colnames(rate.sked) <- "team"
rate.sked$team2 <- "nobody"
n <- nrow(rate.sked)+1
xgmodel <- model
xgmodel$parameters$attack[n] <- 0
names(xgmodel$parameters$attack[n]) <- c("nobody")
newnames <- names(xgmodel$parameters$attack)
newnames[n] <- "nobody"
names(xgmodel$parameters$attack) <- newnames
xgmodel$parameters$defense[n] <- 0
newnames <- names(xgmodel$parameters$defense)
newnames[n] <- "nobody"
names(xgmodel$parameters$defense) <- newnames
xgmodel$all_teams[n] <- "nobody"
rate.preds <- predict_expg(xgmodel, team1=rate.sked$team, team2=rate.sked$team2, return_df = TRUE)
rate.preds.away <- predict_expg(xgmodel, team1=rate.sked$team2, team2=rate.sked$team, return_df = TRUE)
rate.preds.away$team1 <- rate.preds.away$team2
rate.preds <- rate.preds %>% select(-team2) %>% left_join(rate.preds.away, by = "team1")
rate.preds$expg1 <- (rate.preds$expg1.x + rate.preds$expg2.y)/2
rate.preds$expg2 <- (rate.preds$expg2.x + rate.preds$expg1.y)/2
xgols <- rate.preds %>% select(team1, expg1, expg2) %>% rename(
team = team1,
off.rank = expg1,
def.rank = expg2)
# FutStat.cat
source("~/Google Drive/Futbol/futstat.R")
library(tidyverse)
library(lubridate)
# Read in stats and fix times
swimstats <- read_csv("~/Google Drive/Personal Documents/swimstats.csv")
swimstats$time <- duration(as.numeric(sub("s.*", "", swimstats$time)), "seconds")
swimstats$fast.50 <- duration(as.numeric(sub("s.*", "", swimstats$fast.50)), "seconds")
swimstats$fast.100 <- duration(as.numeric(sub("s.*", "", swimstats$fast.100)), "seconds")
swimstats$fast.200 <- duration(as.numeric(sub("s.*", "", swimstats$fast.200)), "seconds")
# Add new swim
newswim <- tibble(as_date("2020-09-14"), # Date
duration(60*32+19, "seconds"), # Time
1225, # Meters
299, # Calories
duration(51.2, "seconds"), # Fastest 50m Freestyle
duration(60*2+02.6, "seconds"), # Fastest 100m Freestyle
NA, # Fastest 200m Freestyle
700, # Longest Continuous Distance
2.38, # Meters Per Stroke (DPS)
2.94) # Seconds Per Stroke (Stroke Rate)
newswim[1,11] <- newswim[1,9]/newswim[1,10] # Meters Per Second
swimstats[(nrow(swimstats)+1),] <- newswim
# Overwrite csv
write_csv(swimstats,"~/Google Drive/Personal Documents/swimstats.csv")
# PRs and Totals
pr50 <- swimstats %>% arrange(fast.50) %>% select(date,fast.50)
names(pr50) <- c("date","pr")
pr50 <- pr50[1,]
pr100 <- swimstats %>% arrange(fast.100) %>% select(date,fast.100)
names(pr100) <- c("date","pr")
pr100 <- pr100[1,]
pr200 <- swimstats %>% arrange(fast.200) %>% select(date, fast.200)
names(pr200) <- c("date","pr")
pr200 <- pr200[1,]
pr <- rbind(pr50,pr100,pr200)
pr$category <- c("Fastest 50m","Fastest 100m","Fastest 200m")
pr <- pr %>% select(category, pr,date)
pr$days.since <- Sys.Date() - pr$date
prdist <- swimstats %>% arrange(desc(meters)) %>% select(date,meters)
names(prdist) <- c("date","pr")
prdist <- prdist[1,]
prcont <- swimstats %>% arrange(desc(long)) %>% select(date,long)
names(prcont) <- c("date","pr")
prcont <- prcont[1,]
prspeed <- swimstats %>% arrange(desc(meters.per.sec)) %>% select(date,meters.per.sec)
names(prspeed) <- c("date","pr")
prspeed <- prspeed[1,]
pr2 <- rbind(prdist,prcont,prspeed)
pr2$category <- c("Total Distance","Cont. Distance","Meters/Second")
pr2 <- pr2 %>% select(category, pr,date)
pr2$days.since <- Sys.Date() - pr2$date
pr
pr2
# Plots
#ggplot(swimstats %>% filter(date>as.Date("2020-03-01"))) + geom_point(aes(date,meters.per.sec))
#ggplot(swimstats) + geom_point(aes(calories,meters))
View(swimstats)
