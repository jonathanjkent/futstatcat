toptenlocs <- data %>% filter(grid.sq %in% topten$grid.sq)
View(toptenlocs)
toptenlocs <- data %>% filter(grid.sq %in% c("-0.025x39.925", "-0.025x39.95 "))
by.sq <- data %>% group_by(grid.sq) %>% count() %>% arrange(desc(n))
by.sq
topten <- by.sq[1:10,1]
toptenlocs <- data %>% filter(grid.sq %in% topten$grid.sq)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
spain <-  get_map("Madrid, Spain", zoom = 4)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
View(topten)
spain <-  get_map("Madrid, Spain", zoom = 6)
topten <- by.sq[1:100,1]
toptenlocs <- data %>% filter(grid.sq %in% topten$grid.sq)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
spain <-  get_map("Zaragoza, Spain", zoom = 6)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
spain <-  get_map("Valencia, Spain", zoom = 6)
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = toptenlocs)
View(by.sq)
test.sq <- data %>% filter(grid.sq == "2.05x41.55")
ggmap(spain) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
bcn <-  get_map("Barcelona, Spain", zoom = 12)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
bcn <-  get_map("Barcelona, Spain", zoom = 10)
ggmap(bcn) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
sabadell <-  get_map("Sabadell, Spain", zoom = 13)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
sabadell <-  get_map("Sabadell, Spain", zoom = 12)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
sabadell <-  get_map("Torrebonica, Spain", zoom = 13)
ggmap(sabadell) + geom_point(aes(x = masked_lon, y = masked_lat), data = test.sq)
View(test.sq)
test.sq$fake_lon <- test.sq$masked_lon - runif(1, 0, .025)
test.sq$fake_lat <- test.sq$masked_lat - runif(1, 0, .025)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq)
test.sq$fake_lat <- test.sq$masked_lat + runif(1, 0, .025)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq)
test.sq$fake_lon <- test.sq$masked_lon - runif(nrow(test.sq), 0, .025)
test.sq$fake_lat <- test.sq$masked_lat + runif(nrow(test.sq), 0, .025)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq,alpha = 0.05)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq, alpha = 0.2)
ggmap(sabadell) + geom_point(aes(x = fake_lon, y = fake_lat), data = test.sq, alpha = 0.1)
data$fake_lon <- data$masked_lon - runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.1)
bcn <-  get_map("Barcelona, Spain", zoom = 15)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.1)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
bcn <-  get_map("Barcelona, Spain", zoom = 14)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.4)
bcn <-  get_map("Barcelona, Spain", zoom = 13)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.4)
ggmap(bcn) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
madrid <-  get_map("Madrid, Spain", zoom = 13)
ggmap(madrid) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
madrid <-  get_map("Madrid, Spain", zoom = 11)
ggmap(madrid) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
ggmap(madrid) + geom_point(aes(x = fake_lon, y = fake_lat), data = data, alpha = 0.2)
View(data)
library(tidyverse)
library(rvest)
d <- read_html("https://www.teenvogue.com/news-politics") %>% html_nodes(.byline-contributor-link)
d <- read_html("https://www.teenvogue.com/news-politics") %>% html_nodes(".byline-contributor-link")
d
library(tidyverse)
library(sf)
# Load Data as SF Objects
data <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
data$fake_lon <- data$masked_lon + runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
points <- data %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
polys <- st_transform(polys, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Point in polygon function
polys$pt_count <- lengths(st_intersects(polys, points))
View(polys)
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
View(income)
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
View(income)
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
income$Renta_Hogar <- as.numeric(income$Renta_Hogar)
income$Renta_Persona <- as.numeric(income$Renta_Persona)
View(polys)
?sep
income <- income %>% separate(Area, c("CUSEC", "NMUN"))
income
# Add to ploys df
ploys <- polys %>% left_join(income, by = "CUSEC")
summary(polys)
summary(ploys)
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(tidyverse)
library(sf)
# Load Data as SF Objects
data <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
data$fake_lon <- data$masked_lon + runif(nrow(data), 0, .025)
data$fake_lat <- data$masked_lat + runif(nrow(data), 0, .025)
points <- data %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
polys <- st_transform(polys, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Point in polygon function
polys$pt_count <- lengths(st_intersects(polys, points))
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
#municipalities <- st_make_valid(polys) %>% group_by(NMUN) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
# Load income data
income <- read_csv("~/research/incomebybarri.csv")
income$Renta_Persona <- as.numeric(income$Renta_Persona)
income$Renta_Hogar <- as.numeric(income$Renta_Hogar)
income <- income %>% separate(Area, c("CUSEC", "NMUN"))
# Add to ploys df
polys <- polys %>% left_join(income, by = "CUSEC")
library(ggplot2)
ggplot(polys,aes(Renta_Hogar,pt_count))
ggplot(polys,aes(Renta_Hogar,pt_count)) + geom_point()
# Twitter data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets
points <- data %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
polys$tw_count <- lengths(st_intersects(polys, tweets))
summary(polys)
ggplot(polys,aes(tw_count,pt_count)) + geom_point()
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(data), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
ggplot(districts,aes(tw_pct,bt_pct)) + geom_point()
districts$more_tweets <- districts$tw_pct - districts$bt_pct
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point()
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
ggplot(filter(districts, tw_count>0),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
filter(districts, tw_count>0),
filter(districts, tw_count>0)
filter(districts, tw_count>10)
ggplot(filter(districts, tw_count>10),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
View(districts)
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
View(tweets)
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
View(districts)
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$more_tweets <- districts$tw_pct - districts$bt_pct
# Load and add income data to district data
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN2"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Collapse by community or municipality
#communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise(bt_count = sum(bt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
munitweets <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(tw_count = sum(tw_count))
municipalities <- municipalities %>% left_join(munitweets, by = "NMUN")
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
municipalities
municipalities$bt_pct <- municipalities$bt_count/sum(municipalities$bt_count)
municipalities$tw_pct <- municipalities$tw_count/sum(municipalities$tw_count)
municipalities$more_tweets <- municipalities$tw_pct - municipalities$bt_pct
View(municipalities)
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$more_tweets <- communities$tw_pct - communities$bt_pct
View(communities)
communities
communities %>% arrange(more_tweets)
municipalities %>% arrange(more_tweets)
# CSV exports
write_csv(communities, "~/research/communities.csv")
write_csv(municipalities, "~/research/municipalities.csv")
municipalities %>% arrange(desc(more_tweets)
)
# Names of all required packages that we will be using during the course
all_pkgs <- c('tidyverse', 'jtools', 'sjPlot','ggplot2','ggthemes','haven','foreign','essurvey','stargazer','knitr','prais','orcutt','fastDummies')
# Install all the packages (it may take a few minutes to install all of them)
install.packages(all_pkgs, dependencies = TRUE)
# Verify the installation worked correctly
setdiff(all_pkgs, row.names(installed.packages()))
library(essurvey)
?essurvey
library(tidyverse)
library(rvest)
d <- read_html("https://mykbostats.com/stats/matrix") %>% html_table()
d
d <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data_frame()
d <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data.frame()
d
pivot_longer(d, names_to = "opp", values_to = "record")
pivot_longer(d[,2:11], names_to = "opp", values_to = "record")
pivot_longer(d[2:11],, names_to = "opp", values_to = "record")
d %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record")
d <- d %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record")
d$opp <- sub(".", " ", d$opp)
d
d <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data.frame()
d <- d %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record")
d
sub("*.*", " ", d$opp)
sub(".", " ", d$opp)
gsub(".", " ", d$opp, fixed=TRUE)
d$opp <- gsub(".", " ", d$opp, fixed=TRUE)
d
library(tidyverse)
library(rvest)
d <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data.frame()
d <- d %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record") %>% filter(record != "—")
d$opp <- gsub(".", " ", d$opp, fixed=TRUE)
d
tiebreaks <- read_csv("ties2020.csv", col_types = cols())
tiebreaks <- read_csv("~/Google Drive/KBO/ties2020.csv", col_types = cols())
tiebreaks
substring(d$record, first = 2, last=5)
substring(d$record, first = 2, last=4)
d$record <- substring(d$record, first = 2, last=4)
d
d <- d %>% mutate(value = ifelse(record > 500,2,ifelse(record < 500,-1,0)))
d
d$tieteam <- paste(d$W.L,d$opp)
d
d <- d %>% select(tieteam,value)
d
tiebreaks
print("Breaking ties...")
tiebreaks <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data.frame()
tiebreaks <- tiebreaks %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record") %>% filter(record != "—")
tiebreaks$opp <- gsub(".", " ", tiebreaks$opp, fixed=TRUE)
tiebreaks$record <- substring(tiebreaks$record, first = 2, last=4)
tiebreaks <- tiebreaks %>% mutate(value = ifelse(record > 500,2,ifelse(record < 500,-1,0)))
tiebreaks$tieteam <- paste(tiebreaks$W.L,tiebreaks$opp)
tiebreaks <- tiebreaks %>% select(tieteam,value)
tiebreaks <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data.frame()
tiebreaks <- tiebreaks %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record") %>% filter(record != "—")
tiebreaks$opp <- gsub(".", " ", tiebreaks$opp, fixed=TRUE)
tiebreaks$record <- substring(tiebreaks$record, first = 2, last=4)
tiebreaks <- tiebreaks %>% mutate(value = ifelse(record > 500,2,ifelse(record < 500,-1,0)))
tiebreaks$tieteam <- paste(tiebreaks$W.L,tiebreaks$opp)
tiebreaks <- tiebreaks %>% select(tieteam,value)
tiebreaks
suppressMessages(library(blogdown))
suppressMessages(library(rmarkdown))
suppressMessages(library(tidyverse))
suppressMessages(library(rvest))
suppressMessages(library(elo))
suppressMessages(library(scales))
options(dplyr.summarise.inform=F)
setwd("~/Google Drive/KBO")
## Scrape 2020 Schedule and Results
links <- read_csv("mykboweeks.csv", col_types = cols())
links$date <- as.Date(str_sub(links$url,-10,-1))
links <- links %>% filter(date < (Sys.Date()+1))
links <- links$url
all.results <- NA
for (i in links){
print(paste0("Scraping: ",i))
url <- i
week <- substring(url, first = 38)
d <- read_html(url) %>% html_table() %>% as.data.frame() %>% select(-X2, -X4)
d <- d[2:(nrow(d)-1),]
d <- d %>% filter(str_detect(X1, "2020") == F)
d <- d %>% separate(X1, c("away1", "away2"), extra = "drop")
d <- d %>% separate(X5, c("home1", "home2"), extra = "drop")
d <- d %>% separate(X3, c("A.Score", "H.Score"), extra = "drop")
d$Away <- paste(d$away1, d$away2)
d$Home <- paste(d$home1, d$home2)
d$week <- as.Date(week)
d$Year <- 2020
d$addday <- rep(0:6, each = 5, length.out = nrow(d))
d$Date <- as.Date(d$week + d$addday)
results <- d %>% filter(!str_detect(H.Score, "0am") & !str_detect(A.Score, "Canceled")) %>% select(Date, Home, Away, H.Score, A.Score, Year)
all.results <- rbind(all.results, results)
}
all.results <- all.results[2:nrow(all.results),]
## Build complete set of 720 games
all.schedule <- as.data.frame(rep(unique(all.results$Home), each = 10))
all.schedule$Away <- rep(unique(all.results$Home), times = 10)
colnames(all.schedule)[1] <- "Home"
all.schedule <- all.schedule %>% filter(Home != Away)
all.schedule <- data.frame(all.schedule, Matchup = rep(1:8, each = 90))
all.schedule$key <- paste(all.schedule$Home,all.schedule$Away,all.schedule$Matchup)
## Remove Completed Games
completed <- data.frame(paste(all.results$Home,all.results$Away))
colnames(completed)[1] <- "Matchup"
completed <- completed %>% group_by(Matchup) %>% mutate(key=row_number())
completed$Matchup <- paste(completed$Matchup,completed$key)
completed <- as.character(completed$Matchup)
all.schedule <- all.schedule %>% filter(!key %in% completed) %>% select(Home, Away)
## Tally Current Win Totals
all.results$H.Score <- as.numeric(all.results$H.Score)
all.results$A.Score <- as.numeric(all.results$A.Score)
all.results <- mutate(all.results, H.Win = ifelse(H.Score > A.Score, 1,
ifelse(H.Score == A.Score, .5, 0)))
all.results <- mutate(all.results, A.Win = ifelse(H.Score < A.Score, 1,
ifelse(H.Score == A.Score, .5, 0)))
h.wins <- all.results %>% select(Home, H.Win)
names(h.wins) <- c("Winner", "n")
a.wins <- all.results %>% select(Away, A.Win)
names(a.wins) <- c("Winner", "n")
wins <- rbind(h.wins,a.wins)
wins <- wins %>% group_by(Winner) %>% summarize(n = sum(n))
all.results <- all.results %>% select(-H.Win, -A.Win)
## Read In Historical Results
kbohistory <- read_csv("kboresults.csv", col_types = cols())
kbohistory$Date <- ISOdate(kbohistory$Year, kbohistory$Month, kbohistory$Day)
kbohistory <- kbohistory %>% select(Date, Home, Away, H.Score, A.Score, Year)
all.history <- rbind(kbohistory, all.results)
all.history <- all.history[order(all.history$Date),]
all.history$H.Score <- as.double(all.history$H.Score)
all.history$A.Score <- as.double(all.history$A.Score)
## Run Elo
print("Running model...")
elo <- elo.run(score(H.Score, A.Score) ~ adjust(Home, 24) + Away + regress(Year, 1500, 0.5) + k(4*(abs(H.Score - A.Score)^(1/4))), data = all.history)
all.history$H.Elo <- elo[[1]][,7]
all.history$A.Elo <- elo[[1]][,8]
## Create Elo History Table
home <- all.history %>% select(Date, Home, H.Elo)
away <- all.history %>% select(Date, Away, A.Elo)
cols <- c("Date", "Team", "Elo")
colnames(home) <- cols
colnames(away) <- cols
elo.history <- rbind(home,away)
elo.history$Elo <- round(elo.history$Elo, digits = 0)
## Current Elos
current.elos <- elo.history %>%
group_by(Team) %>%
arrange(desc(Date)) %>%
slice(1) %>% select(-Date) %>%
arrange(desc(Elo))
current.elos$Rank <- 1:10
write_csv(current.elos, "currentelos.csv")
## Ten Team Era for History Page
elo.history$Date <- as.Date(elo.history$Date)
tenteam <- elo.history %>% filter(Date > as.Date("2013-01-01"))
write_csv(tenteam,"elohistory.csv")
## Simulations
print("Simulating...")
nsim <- 100000
all.schedule$Prediction <- predict(elo, newdata = all.schedule)
all.schedule <- data.frame(all.schedule, Round = rep(1:nsim, each = nrow(all.schedule)))
all.schedule$Sim <- runif((nrow(all.schedule)),0,1)
all.schedule <- mutate(all.schedule, H.Win = ifelse(Sim < Prediction, 1, 0))
all.schedule <- mutate(all.schedule, Winner = ifelse(H.Win == 1, as.character(Home), as.character(Away)))
standings <- all.schedule %>% group_by(Winner, Round) %>% count()
if (nrow(standings) != nsim*10){
key1 <- paste0(standings$Winner,standings$Round)
print(paste0("Simulations with winless teams: ",nsim*10-length(key1)))
key2 <- data.frame(current.elos$Team,rep(1:nsim,each = 10))
key2$key <- paste0(key2$current.elos.Team, key2$rep.1.nsim..each...10.)
key3 <- key2 %>% filter(!key %in% key1)
key3$key <- 0
names(key3) <- names(standings)
standings <- rbind(standings,key3)
}
standings <- standings %>% left_join(wins, by = "Winner")
standings$n.y[is.na(standings$n.y)] <- 0
standings$n <- standings$n.x + standings$n.y
standings$tiebreak <- runif((nrow(standings)),0,1)
############ Tiebreakers ################
print("Breaking ties...")
tiebreaks <- read_html("https://mykbostats.com/stats/matrix") %>% html_table() %>% data.frame()
tiebreaks <- tiebreaks %>% pivot_longer(c("NC.Dinos","LG.Twins","Doosan.Bears","KT.Wiz","Kiwoom.Heroes","Kia.Tigers","Lotte.Giants","Samsung.Lions","SK.Wyverns","Hanwha.Eagles"), names_to = "opp", values_to = "record") %>% filter(record != "—")
tiebreaks$opp <- gsub(".", " ", tiebreaks$opp, fixed=TRUE)
tiebreaks$record <- substring(tiebreaks$record, first = 2, last=4)
tiebreaks <- tiebreaks %>% mutate(value = ifelse(record > 500,2,ifelse(record < 500,-1,0)))
tiebreaks$tieteam <- paste(tiebreaks$W.L,tiebreaks$opp)
tiebreaks <- tiebreaks %>% select(tieteam,value)
standings <- standings %>% arrange(desc(n)) %>% arrange(Round)
standings$tieteam <- "x"
standings$tieteam[1:(length(standings$tieteam)-1)] <- standings$Winner[2:length(standings$tieteam)]
standings$tieteam <- paste(standings$Winner, standings$tieteam)
standings
standings <- standings %>% left_join(tiebreaks, by = "tieteam")
standings
standings$value[is.na(standings$value)]
standings$tiewins <- "x"
standings$tiewins[1:(length(standings$tiewins)-1)] <- standings$n[2:length(standings$n)]
standings
View(standings)
View(standings)
standings <- standings %>% mutate(tiebreak = ifelse(n == tiewins, value, tiebreak))
print("Ranking...")
standings <- standings %>% arrange(tiebreak) %>% arrange(desc(n)) %>% arrange(Round)
standings$Rank <- rep(1:10, max(standings$Round))
pred.1 <- standings %>% filter(Rank == 1) %>% group_by(Winner) %>% count()
pred.1$Pct <- (pred.1$n/nsim)
colnames(pred.1) <- c("Winner", "n", "First")
pred.2 <- standings %>% filter(Rank == 2) %>% group_by(Winner) %>% count()
pred.2$Pct <- (pred.2$n/nsim)
colnames(pred.2) <- c("Winner", "n", "Second")
pred.3 <- standings %>% filter(Rank == 3) %>% group_by(Winner) %>% count()
pred.3$Pct <- (pred.3$n/nsim)
colnames(pred.3) <- c("Winner", "n", "Third")
pred.4 <- standings %>% filter(Rank == 4) %>% group_by(Winner) %>% count()
pred.4$Pct <- (pred.4$n/nsim)
colnames(pred.4) <- c("Winner", "n", "Fourth")
pred.5 <- standings %>% filter(Rank == 5) %>% group_by(Winner) %>% count()
pred.5$Pct <- (pred.5$n/nsim)
colnames(pred.5) <- c("Winner", "n", "Fifth")
pred.6 <- standings %>% filter(Rank > 5) %>% group_by(Winner) %>% count()
pred.6$Pct <- (pred.6$n/nsim)
colnames(pred.6) <- c("Winner", "n", "Out")
predictions <- ungroup(as.data.frame(standings$Winner[1:10]))
colnames(predictions) <- "Winner"
predictions <- pred.1 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.2 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.3 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.4 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.5 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions <- pred.6 %>% select(-n) %>% right_join(predictions, by = "Winner")
predictions$Team <- predictions$Winner
predictions <- predictions %>% left_join(current.elos, by = "Team")
predictions <- predictions %>% ungroup() %>% select(-Winner)
predictions <- predictions %>%
mutate(First = percent(First, 1)) %>%
mutate(Second = percent(Second, 1)) %>%
mutate(Third = percent(Third, 1)) %>%
mutate(Fourth = percent(Fourth, 1)) %>%
mutate(Fifth = percent(Fifth, 1)) %>%
mutate(Out = percent(Out, 1))
predictions <- predictions %>% select(Rank, Team, Elo, First, Second, Third, Fourth, Fifth, Out)
predictions[is.na(predictions)] <- "0%"
predictions
View(predictions)
# KBO Fancy Stats
source("~/Google Drive/KBO/2020elo.R")
source("~/Google Drive/KBO/2020stats.R")
source("~/Google Drive/Futbol/c19update.R")
