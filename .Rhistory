# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(data), 0, .025)
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
ggplot(districts,aes(tw_pct,bt_pct)) + geom_point()
districts$more_tweets <- districts$tw_pct - districts$bt_pct
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point()
ggplot(districts,aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
ggplot(filter(districts, tw_count>0),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
filter(districts, tw_count>0),
filter(districts, tw_count>0)
filter(districts, tw_count>10)
ggplot(filter(districts, tw_count>10),aes(more_tweets,renta_hogar)) + geom_point() + geom_smooth()
View(districts)
# Collapse by community or municipality
#communities <- st_make_valid(polys) %>% group_by(NCA) %>% summarise(obs = sum(pt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
View(tweets)
library(tidyverse)
library(sf)
library(ggplot2)
# Load background track data
bgtracks <- read_csv("~/research/data/exp_raw/tigaserver_app_fix.csv")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- readRDS("~/research/sfm_export_b4f78871b13d4773a0bcde6cb2c348db.rds")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
View(districts)
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$more_tweets <- districts$tw_pct - districts$bt_pct
# Load and add income data to district data
income <- read_csv("~/research/incomebybarri.csv")
income$renta_persona <- as.numeric(income$renta_persona)
income$renta_hogar <- as.numeric(income$renta_hogar)
income <- income %>% separate(area, c("CUSEC", "NMUN2"))
districts <- districts %>% left_join(income, by = "CUSEC")
# Collapse by community or municipality
#communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise(bt_count = sum(bt_count)) %>% arrange(desc(obs))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count)) %>% summarise(tw_count = sum(tw_count))
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(bt_count = sum(bt_count))
munitweets <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise(tw_count = sum(tw_count))
municipalities <- municipalities %>% left_join(munitweets, by = "NMUN")
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
municipalities
municipalities$bt_pct <- municipalities$bt_count/sum(municipalities$bt_count)
municipalities$tw_pct <- municipalities$tw_count/sum(municipalities$tw_count)
municipalities$more_tweets <- municipalities$tw_pct - municipalities$bt_pct
View(municipalities)
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$more_tweets <- communities$tw_pct - communities$bt_pct
View(communities)
communities
communities %>% arrange(more_tweets)
municipalities %>% arrange(more_tweets)
# CSV exports
write_csv(communities, "~/research/communities.csv")
write_csv(municipalities, "~/research/municipalities.csv")
municipalities %>% arrange(desc(more_tweets)
)
# Names of all required packages that we will be using during the course
all_pkgs <- c('tidyverse', 'jtools', 'sjPlot','ggplot2','ggthemes','haven','foreign','essurvey','stargazer','knitr','prais','orcutt','fastDummies')
# Install all the packages (it may take a few minutes to install all of them)
install.packages(all_pkgs, dependencies = TRUE)
# Verify the installation worked correctly
setdiff(all_pkgs, row.names(installed.packages()))
library(essurvey)
?essurvey
library(tidyverse)
library(arrow)
first <- 901
last <- 1035
start <- Sys.time()
for (i in first:last){
filestart <- Sys.time()
file <- paste0("/Volumes/research/data_run2/chunk_",i,".parquet")
raw <- read_parquet(file) %>%
select(created_at, user.id, coordinates.coordinates) %>%
filter(map_lgl(coordinates.coordinates, ~ !is.null(.)))
raw <- suppressMessages(unnest_wider(raw, coordinates.coordinates))
names(raw) <- c("timestamp","user","lon","lat")
csv_name <- paste0("/Volumes/research/processed_tweeets/chunk_",i,"_pro.csv")
write_csv(raw, csv_name)
raw <- raw %>%
filter(lon > -9.39288367353 & lon < 3.03948408368) %>%
filter(lat > 35.946850084 & lat < 43.7483377142)
spain_name <- paste0("~/research/spain_tweets/spain_",i,".csv")
write_csv(raw, spain_name)
fileend <- Sys.time()
print(paste0("Chunk ",i," Completed | ",round((i-first)/(last-first)*100,1),"% of Set"))
print(paste0("Date range: ", min(raw$timestamp), " to ", max(raw$timestamp)))
print(fileend-filestart)
}
end <- Sys.time()
end-start
suppressMessages(library(blogdown))
suppressMessages(library(rmarkdown))
suppressMessages(library(tidyverse))
suppressMessages(library(rvest))
suppressMessages(library(elo))
suppressMessages(library(scales))
options(dplyr.summarise.inform=F)
setwd("~/Google Drive/KBO")
## Scrape 2020 Schedule and Results
links <- read_csv("mykboweeks.csv", col_types = cols())
links$date <- as.Date(str_sub(links$url,-10,-1))
links <- links %>% filter(date < (Sys.Date()+1))
links <- links$url
all.results <- NA
for (i in links){
print(paste0("Scraping: ",i))
url <- i
week <- substring(url, first = 38)
d <- read_html(url) %>% html_table() %>% as.data.frame() %>% select(-X2, -X4)
d <- d[2:(nrow(d)-1),]
d <- d %>% filter(str_detect(X1, "2020") == F)
d <- d %>% separate(X1, c("away1", "away2"), extra = "drop")
d <- d %>% separate(X5, c("home1", "home2"), extra = "drop")
d <- d %>% separate(X3, c("A.Score", "H.Score"), extra = "drop")
d$Away <- paste(d$away1, d$away2)
d$Home <- paste(d$home1, d$home2)
d$week <- as.Date(week)
d$Year <- 2020
d$addday <- rep(0:6, each = 5, length.out = nrow(d))
d$Date <- as.Date(d$week + d$addday)
results <- d %>% filter(!str_detect(H.Score, "0am") & !str_detect(A.Score, "Canceled")) %>% select(Date, Home, Away, H.Score, A.Score, Year)
all.results <- rbind(all.results, results)
}
all.results <- all.results[2:nrow(all.results),]
View(all.results)
## Read In Historical Results
kbohistory <- read_csv("kboresults.csv", col_types = cols())
kbohistory$Date <- ISOdate(kbohistory$Year, kbohistory$Month, kbohistory$Day)
kbohistory <- kbohistory %>% select(Date, Home, Away, H.Score, A.Score, Year)
all.history <- rbind(kbohistory, all.results)
all.history <- all.history[order(all.history$Date),]
all.history$H.Score <- as.double(all.history$H.Score)
all.history$A.Score <- as.double(all.history$A.Score)
## Run Elo
print("Running model...")
elo <- elo.run(score(H.Score, A.Score) ~ adjust(Home, 24) + Away + regress(Year, 1500, 0.5) + k(4*(abs(H.Score - A.Score)^(1/4))), data = all.history)
all.history$H.Elo <- elo[[1]][,7]
all.history$A.Elo <- elo[[1]][,8]
## Create Elo History Table
home <- all.history %>% select(Date, Home, H.Elo)
away <- all.history %>% select(Date, Away, A.Elo)
cols <- c("Date", "Team", "Elo")
colnames(home) <- cols
colnames(away) <- cols
elo.history <- rbind(home,away)
elo.history$Elo <- round(elo.history$Elo, digits = 0)
## Current Elos
current.elos <- elo.history %>%
group_by(Team) %>%
arrange(desc(Date)) %>%
slice(1) %>% select(-Date) %>%
arrange(desc(Elo))
current.elos$Rank <- 1:10
write_csv(current.elos, "currentelos.csv")
## Ten Team Era for History Page
elo.history$Date <- as.Date(elo.history$Date)
tenteam <- elo.history %>% filter(Date > as.Date("2013-01-01"))
write_csv(tenteam,"elohistory.csv")
# Update Elo page
predictions <- read_csv("predictions.csv", col_types = cols()) %>% select(-Elo, -Rank)
predictions <- predictions %>% left_join(current.elos, by = "Team") %>% select(Rank, Team, Elo, everything())
write_csv(predictions, "predictions.csv")
i<-1
all.presults <- NA
psim <- 10000
print("Simulating playoffs...")
seed1 <- "NC Dinos"
seed2 <- "KT Wiz"
seed3 <- "Doosan Bears"
seed4 <- "LG Twins"
seed5 <- "Kiwoom Heroes"
# Wild Card Round
#psked <- data.frame(seed4,seed5)
#psked <- rbind(psked,psked,psked)
#names(psked) <- c("Home","Away")
#psked$Prediction <- predict(elo, newdata = psked)
#psked$Sim <- runif((nrow(psked)),0,1)
#psked[1,4] <- 1
#psked <- mutate(psked, Winner = ifelse(Sim > Prediction, as.character(Home), as.character(Away)))
#wc <- row.names(data.frame(sort(table(psked$Winner),decreasing=TRUE)[1]))
#fifth <- ifelse(wc == seed4, seed5, seed4)
wc <- seed4
fifth <- seed5
psked <- data.frame(seed3,wc)
names(psked) <- c("Home","Away")
away <- data.frame(wc,seed3)
names(away) <- c("Home","Away")
psked <- rbind(psked,psked,away)
psked$Prediction <- predict(elo, newdata = psked)
psked$Sim <- runif((nrow(psked)),0,1)
psked <- mutate(psked, Winner = ifelse(Sim > Prediction, as.character(Home), as.character(Away)))
psked
psked[1,5]
# KBO Fancy Stats
source("~/Google Drive/KBO/2020playoff.R")
# FutStat.cat
source("~/Google Drive/Futbol/futstat.R")
# KBO Fancy Stats
source("~/Google Drive/KBO/2020playoff.R")
200*.75-200*.25=100
200*.75-200*.25==100
100/400
100/100
100/150
100/170
100/125
100/120
100/140
100/145
100/400
200*.85-200*.15
140/400
200*.85-200*.15
140/200
200*.85
200*.15
140/200
140/200 - 1 * 2
140/200 - 1
1- 140/200
140/200 + ((1 - 140/200)/2)
100/200 + ((1 - 100/200)/2)
200*.75-200*.25 == 100
50/200 + ((1 - 50/200)/2)
200*.625-200*.375 == 50
remaining <- 200
deficit <- 50
deficit/remaining + ((1 - deficit/remaining)/2)
ga.remaining <- 50000
ga.deficit <- 2432424-2413836
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
pa.remaining <- 1000000
pa.deficit <- 3216674-3052577
pa.deficit/pa.remaining + ((1 - pa.deficit/pa.remaining)/2)
pa.remaining <- 400000
pa.deficit <- 3216674-3052577
pa.deficit/pa.remaining + ((1 - pa.deficit/pa.remaining)/2)
library(tidyverse)
library(sf)
# Load background track data
bgtracks <- readRDS("~/research/data/exp_pro/user_locations_small_cell.rds")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- read_csv("~/research/tweets_jul1_sep15.csv")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$bt_vs_tweets <- districts$bt_pct - districts$tw_pct
## Collapse by autonomous community and municipality
# CCAA
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities <- communities %>% filter(NCA != "Pais Vasco")
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$bt_vs_tweets <- communities$bt_pct -communities$tw_pct
ccaadata <- read_csv("~/research/ccaapopincome.csv")
communities <- communities %>% left_join(ccaadata, by = "NCA")
communities$pop_pct <- communities$population/sum(communities$population)
communities$bt_vs_pop <- communities$bt_pct - communities$pop_pct
# CCAA Analysis: no added value using tweets, no relationship w/ income
cor(communities$bt_vs_pop, communities$bt_vs_tweets)
#summary(lm(more_tweets ~ income, data = communities %>% filter(bt_count > 20)))
#ggplot(communities %>% filter(bt_count > 20)) + geom_point(aes(income, more_tweets)) + geom_smooth(aes(income, more_tweets), method = 'lm')
# Municipalities
municipalities <- st_make_valid(districts) %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
municipalities$bt_pct <- municipalities$bt_count/sum(municipalities$bt_count)
municipalities$tw_pct <- municipalities$tw_count/sum(municipalities$tw_count)
municipalities$bt_vs_tweets <- municipalities$bt_pct - municipalities$tw_pct
#st_geometry(municipalities) <- NULL
munidata <- read_csv("~/research/munipopincome.csv")
municipalities <- municipalities %>% left_join(munidata, by = "NMUN")
municipalities$pop_pct <- municipalities$population/sum(municipalities$population, na.rm = T)
municipalities$bt_vs_pop <- municipalities$bt_pct - municipalities$pop_pct
# Muni Analysis: Pop & tweets not as closely correlated, still no relation w/ income
cor(municipalities$bt_vs_pop, municipalities$bt_vs_tweets, use = "complete.obs")
#summary(lm(more_pop ~ more_tweets, data = municipalities %>% filter(bt_count > 0)))
#summary(lm(bt_count ~ income + pop_pct, data = municipalities %>% filter(bt_count > 0)))
#ggplot(municipalities %>% filter(bt_count > 0 & more_tweets < .02 & more_tweets > -.02)) + geom_point(aes(income, more_tweets)) + geom_smooth(aes(income, more_tweets), method = 'lm')
#ggplot(municipalities %>% filter(bt_count > 0 & bt_count < 1500)) + geom_point(aes(income, bt_count)) + geom_smooth(aes(income, bt_count), method = 'lm')
#ggplot(municipalities %>% filter(bt_count > 0 & more_tweets > -.02 & more_tweets < .02)) + geom_point(aes(more_tweets, more_pop)) + geom_smooth(aes(more_tweets, more_pop), method = 'lm')
#municipalities %>% arrange(desc(suburbs)) %>% select(NMUN,suburbs,tw_count)
###
# Load and add income data to district data
income <- read_csv("~/research/incomebybarri.csv") %>% select(-renta_persona)
population <- read_csv("~/research/popbybarri.csv")
population$CUSEC <- as.character(population$CUSEC)
income$renta_hogar <- as.numeric(income$renta_hogar)
names(income)[2] <- "income"
income <- income %>% separate(area, c("CUSEC"))
districts <- districts %>% left_join(income, by = "CUSEC")
districts$income <- districts$income * 1000
districts <- districts %>% left_join(population, by = "CUSEC")
districts$pop_pct <- districts$population/sum(districts$population, na.rm = T)
districts$bt_vs_pop <- districts$bt_pct - districts$pop_pct
# Analysis: no relationship
cor(districts$bt_vs_pop, districts$bt_vs_tweets, use = "complete.obs")
summary(lm(bt_vs_tweets ~ income, data = municipalities))
summary(lm(bt_vs_tweets ~ income, data = districts))
ggplot() +
geom_point(data = municipalities %>% filter(tw_count > 0 & bt_vs_tweets < .02 & bt_vs_tweets > -.02), aes(income.rank, bt_vs_tweets_absolute)) +
geom_smooth(data = municipalities, aes(income.rank, bt_vs_tweets_absolute), method = 'lm') +
theme_minimal() + labs(x="",y="") + scale_x_reverse()
ggplot() +
geom_point(data = districts %>% filter(tw_count > 0 & bt_vs_tweets < .05 & bt_vs_tweets > -.01), aes(income, bt_vs_tweets)) +
geom_smooth(data = districts, aes(income, bt_vs_tweets), method = 'lm') +
theme_minimal() + labs(x="",y="")
# Simplify and save
#st_geometry(communities) <- NULL
#st_geometry(municipalities) <- NULL
#st_geometry(districts) <- NULL
#districts <- districts %>% select(CUSEC, NMUN, bt_count, tw_count, bt_pct, tw_pct, bt_vs_tweets, population, income, pop_pct, bt_vs_pop)
#write_csv(communities, "~/research/ccaa_datasimple.csv")
#write_csv(municipalities, "~/research/muni_datasimple.csv")
#write_csv(districts, "~/research/tract_datasimple.csv")
# Plots and Maps
ccaa <- communities %>% filter(NCA != "Canarias") %>% filter(NCA != "Ceuta") %>% filter(NCA != "Melilla")
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15))
plot(ccaa["bt_vs_pop"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15,.25))
# Madrid
madrid <- st_make_valid(districts) %>% filter(NCA == "Comunidad de Madrid") %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
madrid$bt_pct <- madrid$bt_count/sum(madrid$bt_count)
madrid$tw_pct <- madrid$tw_count/sum(madrid$tw_count)
madrid$bt_vs_tweets <- madrid$bt_pct - madrid$tw_pct
munidata <- read_csv("~/research/munipopincome.csv")
madrid <- madrid %>% left_join(munidata, by = "NMUN")
madrid$pop_pct <- madrid$population/sum(madrid$population, na.rm = T)
madrid$bt_vs_pop <- madrid$bt_pct - madrid$pop_pct
madrid2 <- madrid %>% filter(tw_count > 1 & bt_count > 1 & population > 0)
# BCN
bcn <- st_make_valid(districts) %>% filter(NPRO == "Barcelona") %>% group_by(NMUN) %>% summarise_at(vars(bt_count, tw_count), sum)
bcn$bt_pct <- bcn$bt_count/sum(bcn$bt_count)
bcn$tw_pct <- bcn$tw_count/sum(bcn$tw_count)
bcn$bt_vs_tweets <- bcn$bt_pct - bcn$tw_pct
munidata <- read_csv("~/research/munipopincome.csv")
bcn <- bcn %>% left_join(munidata, by = "NMUN")
bcn$pop_pct <- bcn$population/sum(bcn$population, na.rm = T)
bcn$bt_vs_pop <- bcn$bt_pct - bcn$pop_pct
bcn %>% arrange(desc(bt_count))
bcn2 <- bcn %>% filter(tw_count > 1 & bt_count > 1 & population > 0)
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15))
sf.colors(1)
sf.colors(2)
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), col = "red")
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), col = c("red","blue"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), col = c("#3d3d3d","blue"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = c("#3d3d3d","blue"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = (start = "red", end = 'blue'))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = (start = "red", end = "blue"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = (n = 12, start = "red", end = "blue"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = (n = 12, start = red, end = blue))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = rainbow(n = 12, start = red, end = blue))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = rainbow(n = 12, start = "red", end = "blue"))
hcl.pals()
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(palette = "Dark Mint"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 12, palette = "Dark Mint"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint"))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
# City maps
plot(bcn2["bt_vs_pop"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(bcn2["bt_vs_tweets"], breaks = c(-.43,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(madrid2["bt_vs_pop"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(madrid2["bt_vs_tweets"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(madrid2["bt_vs_tweets"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Mint", rev = T))
plot(madrid2["bt_vs_pop"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Mint", rev = T))
plot(ccaa["bt_vs_tweets"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(ccaa["bt_vs_pop"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15,.25), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(ccaa["bt_vs_pop"],breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15,.25), pal = hcl.colors(n = 12, palette = "Dark Mint", rev = T))
plot(madrid2["bt_vs_pop"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(madrid2["bt_vs_tweets"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(bcn2["bt_vs_pop"], breaks = c(-.2,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
plot(bcn2["bt_vs_tweets"], breaks = c(-.43,-.15,-.10,-.075,-.05,-.025,0,.025,.05,.075,.1,.15), pal = hcl.colors(n = 11, palette = "Dark Mint", rev = T))
ga.remaining <- 61367
ga.deficit <- 2432424-2413836
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
ga.deficit <- 2434292 - 2419435
ga.remaining <- 61367
ga.remaining <- 61367
ga.deficit <- 2434292 - 2419435
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
ga.remaining <- 50000
ga.deficit <- 2434292 - 2419435
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
ga.remaining <- 30000
ga.deficit <- 2434292 - 2419435
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
ga.remaining <- 25000
ga.deficit <- 2434292 - 2419435
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
ga.remaining <- 50401
ga.deficit <- 2434292 - 2419435
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
ga.remaining <- 50401
ga.deficit <- 13540
ga.deficit/ga.remaining + ((1 - ga.deficit/ga.remaining)/2)
# FutStat.cat
source("~/Google Drive/Futbol/futstat.R")
library(tidyverse)
library(sf)
# Load background track data
bgtracks <- readRDS("~/research/data/exp_pro/user_locations_small_cell.rds")
# Randomize location within sampling cell
bgtracks$fake_lon <- bgtracks$masked_lon + runif(nrow(bgtracks), 0, .025)
bgtracks$fake_lat <- bgtracks$masked_lat + runif(nrow(bgtracks), 0, .025)
# Convert points to SF objects
bgtracks <- bgtracks %>% st_as_sf(coords = c("fake_lon", "fake_lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Load districts as SF objects
districts <- st_read("~/research/shapefiles/SECC_CPV_E_20111101_01_R_INE.shp")
districts <- st_transform(districts, "+proj=longlat +ellps=WGS84 +datum=WGS84")
# Add background track count to district data
districts$bt_count <- lengths(st_intersects(districts, bgtracks))
# Load tweets, add tweet count to district data
tweets <- read_csv("~/research/tweets_jul1_sep15.csv")
tweets <- tweets %>% st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84")
districts$tw_count <- lengths(st_intersects(districts, tweets))
# Background tracks and tweets as proportion
districts$bt_pct <- districts$bt_count/sum(districts$bt_count)
districts$tw_pct <- districts$tw_count/sum(districts$tw_count)
districts$bt_vs_tweets <- districts$bt_pct - districts$tw_pct
## Collapse by autonomous community and municipality
# CCAA
communities <- st_make_valid(districts) %>% group_by(NCA) %>% summarise_at(vars(bt_count, tw_count), sum)
communities <- communities %>% filter(NCA != "Pais Vasco")
communities$bt_pct <- communities$bt_count/sum(communities$bt_count)
communities$tw_pct <- communities$tw_count/sum(communities$tw_count)
communities$bt_vs_tweets <- communities$bt_pct -communities$tw_pct
ccaadata <- read_csv("~/research/ccaapopincome.csv")
communities <- communities %>% left_join(ccaadata, by = "NCA")
communities$pop_pct <- communities$population/sum(communities$population)
communities$bt_vs_pop <- communities$bt_pct - communities$pop_pct
# CCAA Analysis: no added value using tweets, no relationship w/ income
cor(communities$bt_vs_pop, communities$bt_vs_tweets)
#summary(lm(more_tweets ~ income, data = communities %>% filter(bt_count > 20)))
#ggplot(communities %>% filter(bt_count > 20)) + geom_point(aes(income, more_tweets)) + geom_smooth(aes(income, more_tweets), method = 'lm')
View(communities)
communities
communities %>% select(NCA, bt_pct, tw_pct, pop_pct)
pa.remaining <- 44000
pa.deficit <- 18000
pa.deficit/pa.remaining + ((1 - pa.deficit/pa.remaining)/2)
pa.remaining <- 160000
pa.deficit <- 18000
pa.deficit/pa.remaining + ((1 - pa.deficit/pa.remaining)/2)
az.remaining <- 263000
az.deficit <- 47000
az.deficit/az.remaining + ((1 - az.deficit/az.remaining)/2)
az.remaining <- 263000
az.deficit <- -47000
az.deficit/az.remaining + ((1 - az.deficit/az.remaining)/2)
